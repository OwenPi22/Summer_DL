{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0f36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946267a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X,train_y), (test_X,test_y) = fashion_mnist.load_data()\n",
    "X = np.concatenate((train_X, test_X))\n",
    "y = np.concatenate((train_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b43745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(70000, 28, 28, 1)\n",
    "y = tf.one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25303ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(70000 * 0.7)\n",
    "test_num = int(70000 * 0.15)\n",
    "val_num = 70000 - train_num - test_num\n",
    "\n",
    "train_X = X[:train_num]\n",
    "val_X = X[train_num : val_num + train_num]\n",
    "test_X = X[val_num + train_num:]\n",
    "\n",
    "train_y = y[:train_num]\n",
    "val_y = y[train_num : val_num + train_num]\n",
    "test_y = y[val_num + train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c320d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d076124",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc986c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH MAXPOOLING\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(14, (3, 3), activation='tanh', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(12, (3, 3), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(10, (3, 3), activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ed2280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 2.5788\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.9739\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.4165\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.3247\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 1.2503\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.9622\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.0793\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.0242\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.9715\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.8547\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.7553\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.0193\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.8697\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.9970\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.7215\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7350\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.8075\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.8653\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6464\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7558\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.7626\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6940\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6584\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.8948\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.8079\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6958\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6063\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7958\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.7775\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6592\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.9208\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7086\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.5447\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6464\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6213\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7549\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 0: 0.5972\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6047\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.9255\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.4079\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    for (x_batch_train, y_batch_train) in datagen.flow(train_X, train_y, batch_size=64):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            losses.append(loss_value)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n",
    "        step += 1\n",
    "\n",
    "        if step >= train_num // 64: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2e3959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7769183673469388"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "pred_train_y = model.predict(train_X)\n",
    "\n",
    "train_labels = []\n",
    "for i in range(0, len(pred_train_y)):\n",
    "    train_labels.append(tf.keras.backend.eval(tf.argmax(train_y[i])))\n",
    "    \n",
    "pred_train_label = []\n",
    "for i in range(0, len(pred_train_y)):\n",
    "    pred_train_label.append(tf.keras.backend.eval(tf.argmax(pred_train_y[i])))\n",
    "    \n",
    "corr = 0\n",
    "for i in range(0, len(pred_train_y)):\n",
    "    if pred_train_label[i] == train_labels[i]:\n",
    "        corr += 1\n",
    "corr / train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad03d29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738095238095238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_y = model.predict(test_X)\n",
    "\n",
    "test_labels = []\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    test_labels.append(tf.keras.backend.eval(tf.argmax(test_y[i])))\n",
    "    \n",
    "pred_test_label = []\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    pred_test_label.append(tf.keras.backend.eval(tf.argmax(pred_test_y[i])))\n",
    "    \n",
    "corr = 0\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    if pred_test_label[i] == test_labels[i]:\n",
    "        corr += 1\n",
    "corr / test_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1bc21",
   "metadata": {},
   "source": [
    "## Similar results obtained from given compile and fit functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3eb8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT MAXPOOLING\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(14, (3, 3), activation='tanh', input_shape=(28, 28, 1)))\n",
    "#model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(12, (3, 3), activation='tanh'))\n",
    "#model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(10, (3, 3), activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6d5ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 766 steps, validate on 10500 samples\n",
      "Epoch 1/10\n",
      "766/766 [==============================] - 104s 135ms/step - loss: 0.8179 - accuracy: 0.6943 - val_loss: 0.5790 - val_accuracy: 0.7658\n",
      "Epoch 2/10\n",
      "766/766 [==============================] - 103s 134ms/step - loss: 0.5941 - accuracy: 0.7715 - val_loss: 0.5237 - val_accuracy: 0.8006\n",
      "Epoch 3/10\n",
      "766/766 [==============================] - 103s 134ms/step - loss: 0.5396 - accuracy: 0.7963 - val_loss: 0.4718 - val_accuracy: 0.8188\n",
      "Epoch 4/10\n",
      "766/766 [==============================] - 102s 134ms/step - loss: 0.5067 - accuracy: 0.8072 - val_loss: 0.5325 - val_accuracy: 0.7958\n",
      "Epoch 5/10\n",
      "766/766 [==============================] - 102s 133ms/step - loss: 0.4727 - accuracy: 0.8216 - val_loss: 0.4749 - val_accuracy: 0.8195\n",
      "Epoch 6/10\n",
      "766/766 [==============================] - 102s 134ms/step - loss: 0.4523 - accuracy: 0.8294 - val_loss: 0.4072 - val_accuracy: 0.8503\n",
      "Epoch 7/10\n",
      "766/766 [==============================] - 105s 137ms/step - loss: 0.4452 - accuracy: 0.8314 - val_loss: 0.4161 - val_accuracy: 0.8437\n",
      "Epoch 8/10\n",
      "766/766 [==============================] - 106s 138ms/step - loss: 0.4286 - accuracy: 0.8378 - val_loss: 0.4093 - val_accuracy: 0.8464\n",
      "Epoch 9/10\n",
      "766/766 [==============================] - 107s 139ms/step - loss: 0.4120 - accuracy: 0.8458 - val_loss: 0.3724 - val_accuracy: 0.8581\n",
      "Epoch 10/10\n",
      "766/766 [==============================] - 102s 133ms/step - loss: 0.4092 - accuracy: 0.8487 - val_loss: 0.5619 - val_accuracy: 0.7853\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_X, train_y, batch_size=64), \n",
    "                    epochs=10, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db8334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828571428571428"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = tf.cast(test_X, tf.float32)\n",
    "pred_test_y = model.predict(test_X)\n",
    "\n",
    "test_labels = []\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    test_labels.append(tf.keras.backend.eval(tf.argmax(test_y[i])))\n",
    "    \n",
    "pred_test_label = []\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    pred_test_label.append(tf.keras.backend.eval(tf.argmax(pred_test_y[i])))\n",
    "    \n",
    "corr = 0\n",
    "for i in range(0, len(pred_test_y)):\n",
    "    if pred_test_label[i] == test_labels[i]:\n",
    "        corr += 1\n",
    "corr / test_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271826a",
   "metadata": {},
   "source": [
    "# No Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715daae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = tf.cast(train_X, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4f370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT MAXPOOLING\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(14, (3, 3), activation='tanh', input_shape=(28, 28, 1)))\n",
    "#model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(12, (3, 3), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "#model.add(layers.Conv2D(10, (3, 3), activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d324139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason the kernel dies here\n",
    "'''\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "losses = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            losses.append(loss_value)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n",
    "        step += 1\n",
    "\n",
    "        if step >= train_num // 64: break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bfb77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49000 samples, validate on 10500 samples\n",
      "Epoch 1/10\n",
      "49000/49000 [==============================] - 68s 1ms/sample - loss: 0.5424 - accuracy: 0.8068 - val_loss: 0.4558 - val_accuracy: 0.8325\n",
      "Epoch 2/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.3875 - accuracy: 0.8596 - val_loss: 0.3869 - val_accuracy: 0.8555\n",
      "Epoch 3/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.3515 - accuracy: 0.8720 - val_loss: 0.3543 - val_accuracy: 0.8722\n",
      "Epoch 4/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.3288 - accuracy: 0.8800 - val_loss: 0.3837 - val_accuracy: 0.8591\n",
      "Epoch 5/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.3065 - accuracy: 0.8885 - val_loss: 0.3232 - val_accuracy: 0.8816\n",
      "Epoch 6/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.2908 - accuracy: 0.8921 - val_loss: 0.3224 - val_accuracy: 0.8809\n",
      "Epoch 7/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.2821 - accuracy: 0.8955 - val_loss: 0.3133 - val_accuracy: 0.8830\n",
      "Epoch 8/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.2764 - accuracy: 0.8976 - val_loss: 0.3165 - val_accuracy: 0.8816\n",
      "Epoch 9/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.2605 - accuracy: 0.9040 - val_loss: 0.3164 - val_accuracy: 0.8829\n",
      "Epoch 10/10\n",
      "49000/49000 [==============================] - 67s 1ms/sample - loss: 0.2511 - accuracy: 0.9077 - val_loss: 0.2891 - val_accuracy: 0.8946\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=10, validation_data=(val_X, val_y), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df87ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
